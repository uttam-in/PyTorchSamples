#!/usr/bin/env mojo
# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

from max.engine import InputSpec, InferenceSession, Model
from python import Python
from tensor import TensorSpec
import sys


def execute(model: Model, text: String, transformers: PythonObject) -> String:
    # The model was compiled with a maximum seqlen, so read that out from the model output metadata
    output_spec = model.get_model_output_metadata()[0]
    max_seqlen = output_spec[1].value()[]

    tokenizer = transformers.AutoTokenizer.from_pretrained("bert-base-uncased")

    inputs = tokenizer(
        text=text,
        add_special_tokens=True,
        padding="max_length",
        truncation=True,
        max_length=max_seqlen,
        return_tensors="np",
    )

    input_ids = inputs["input_ids"]
    token_type_ids = inputs["token_type_ids"]
    attention_mask = inputs["attention_mask"]

    outputs = model.execute(
        "input_ids",
        input_ids,
        "token_type_ids",
        token_type_ids,
        "attention_mask",
        attention_mask,
    )

    logits = outputs.get[DType.float32]("result0")

    mask_idx = -1
    for i in range(len(input_ids[0])):
        if input_ids[0][i] == tokenizer.mask_token_id:
            mask_idx = i

    predicted_token_id = logits.argmax()[mask_idx]
    return tokenizer.decode(
        predicted_token_id,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=True,
    )


def load_model(session: InferenceSession) -> Model:
    batch = 1
    seqlen = 128

    input_ids_spec = TensorSpec(DType.int64, batch, seqlen)
    token_type_ids_spec = TensorSpec(DType.int64, batch, seqlen)
    attention_mask_spec = TensorSpec(DType.int64, batch, seqlen)
    input_specs = List[InputSpec]()

    input_specs.append(input_ids_spec)
    input_specs.append(attention_mask_spec)
    input_specs.append(token_type_ids_spec)

    model = session.load(
        "../../models/bert-mlm.torchscript", input_specs=input_specs
    )

    return model


def read_input() -> String:
    USAGE = (
        'Usage: ./run.mojo <str> \n\t e.g., ./run.mojo "Paris is the [MASK] of'
        ' France"'
    )

    argv = sys.argv()
    if len(argv) != 2:
        raise Error("\nPlease enter a prompt." + "\n" + USAGE)

    return sys.argv()[1]


def main():
    # Import HF Transformers dependency (for the tokenizer)
    transformers = Python.import_module("transformers")

    # Read user prompt, create an InferenceSession, and load the model
    text = read_input()
    session = InferenceSession()
    model = load_model(session)

    # Run inference
    decoded_result = execute(model, text, transformers)

    print("input text: ", text)
    print("filled mask: ", text.replace("[MASK]", decoded_result))
