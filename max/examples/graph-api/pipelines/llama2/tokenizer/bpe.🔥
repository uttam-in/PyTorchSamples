# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""A byte pair encoding tokenizer implementation for use with LLMs."""

from collections import Dict, List, Optional
from pathlib import Path

from ...tokenizer import Tokenizer
from .arena_linked_list import ArenaLinkedList
from .max_heap import MaxHeap, OrderedElement


@value
struct Token(CollectionElement):
    """A token-score pair for storing a BPE vocabulary."""

    var token: String
    var score: Float32


@value
struct MergeOption(OrderedElement):
    """Metadata for tracking possible BPE merges in a priority queue."""

    var left: ArenaLinkedList[String].ID
    var right: ArenaLinkedList[String].ID
    var score: Float32
    var checksum: Int

    fn __lt__(self, other: Self) -> Bool:
        return (self.score < other.score) or (
            self.score == other.score and self.left > other.left
        )


def read[T: CollectionElement](inout span: Span[UInt8, _, _]) -> T:
    """Read a binary type out of a byte buffer and increment the pointer."""
    value = span.unsafe_ptr().bitcast[T]()[]
    span = span[sizeof[T]() :]
    return value^


@value
struct BPETokenizer(Tokenizer):
    """A Byte Pair Encoding string tokenizer.

    [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)
    can tokenize strings of any language or encoding based on a learned input
    dictionary. This implementation may not be fully featured, but is fast
    and compatible with some major LLMs such as Llama. It uses the same
    basic approach as sentencepiece, with an implementation with good
    algorithmic performance, but not fully optimized.
    """

    var vocab: List[Token]
    var token_ids: Dict[String, Int]

    def __init__(inout self):
        self.vocab = List[Token]()
        self.token_ids = Dict[String, Int]()

    @staticmethod
    def from_bytes(data: String) -> BPETokenizer:
        """Construct a BPETokenizer instance given binary token scores.

        The file must have the following binary format:
        ```
        [max_token_len:Int32]
        [score_0:Float32]
        [token_len_0:Int32]
        [token_0:(UInt8*token_len_0)]
        [score_1:Float32]
        [token_len_1:Int32]
        [token_1:(UInt8*token_len_0)]
        ...
        ```
        """

        var model = BPETokenizer()
        span = data.as_bytes_slice()

        _max_token_len = read[Int32](span)
        while len(span):
            score = read[Float32](span)
            token_len = int(read[Int32](span))
            token = String(span[: token_len + 1])
            span = span[token_len:]
            model.add_token(token, score)

        return model^

    @staticmethod
    def from_file(path: Path) -> Self:
        """Construct a BPETokenizer instance given binary token scores.

        See `BPETokenizer.from_bytes()` for the expected file format.
        """
        with open(path, "r") as file:
            return Self.from_bytes(file.read())

    def add_token(inout self, token: String, score: Float32):
        """Add a token to the vocabulary."""
        if token not in self.token_ids:
            self.token_ids[token] = len(self.vocab)
        self.vocab.append(Token(token, score))

    def decode(inout self, output_tokens: List[Int64]) -> String:
        """Decodes a string by indexing the vocabulary."""
        decoded = String()
        for token_id in output_tokens:
            if token_id[] >= len(self.vocab):
                raise "token id: " + str(
                    token_id[]
                ) + " out of vocab size range (" + str(len(self.vocab)) + ")"

            decoded += self.vocab[int(token_id[])].token
        return decoded

    def encode(
        self,
        input_string: List[String],
        bos: Optional[String] = None,
        eos: Optional[String] = None,
    ) -> List[Int64]:
        """Encode a string according to the BPE algorithm.

        The BPE vocabulary is a set of scored strings. BPE starts by
        considering every character in the input string as its own token,
        and then greedily merges the highest scoring adjacent pair
        until no more adjacent token merges exist in the vocabulary.

        We implement the tokens as a linked list, with a priority queue
        of merge options. We execute the highest-scoring merge, adding
        new merge options to the priority queue if they exist in the vocabulary.
        We can't remove out-dated merge options from the priority queue, so
        instead we add a checksum to them, which is the length of the merge
        they're expecting. Linked list elements only stop existing or grow
        in length, so we can always safely recognize an outdated merge.
        """
        if len(input_string) != 1:
            raise "batched encoding not yet supported. Please file an issue"
        str = input_string[0]

        output = List[Int64]()
        if bos and bos.value()[] in self.token_ids:
            output.append(self.token_ids[bos.value()[]])

        merge_options = MaxHeap[MergeOption]()
        tokens = ArenaLinkedList[String]()

        @parameter
        def maybe_add_merge(left: tokens.ID, right: tokens.ID):
            merged = tokens[left] + tokens[right]
            if merged in self.token_ids:
                score = self.vocab[self.token_ids[merged]].score
                merge_options.push(MergeOption(left, right, score, len(merged)))

        # Initialize the tokens linked-list and initial merges.
        var prev: Optional[ArenaLinkedList[String].ID] = None
        for i in range(len(str)):
            id = tokens.append(str[i])
            if prev:
                maybe_add_merge(prev.value()[], id)
            prev = id

        while merge_options:
            merge = merge_options.pop()
            # Check whether the best merge is still valid
            if merge.left not in tokens or merge.right not in tokens:
                continue  # outdated merge option
            merged = tokens[merge.left] + tokens[merge.right]
            if len(merged) != merge.checksum:
                continue  # outdated merge option
            # Merge the right token into the left token, then
            # add any new valid merge options to the priority queue.
            left = tokens.prev(merge.left)
            right = tokens.next(merge.right)
            tokens[merge.left] = merged
            tokens.remove(merge.right)
            if right:
                maybe_add_merge(merge.left, right.value()[])
            if left:
                maybe_add_merge(left.value()[], merge.left)

        # Loop through the final list and construct the token sequence.
        node_id = tokens._head
        while node_id:
            id = node_id.value()[]
            token = tokens[id]
            output.append(self._encode_token(token))
            node_id = tokens.next(id)

        if eos and eos.value()[] in self.token_ids:
            output.append(self.token_ids[eos.value()[]])

        return output

    def _encode_token(self, token: String) -> Int:
        return self.token_ids.find(token).or_else(0)

    def is_end_of_text(self, val: Int64) -> Bool:
        return False
