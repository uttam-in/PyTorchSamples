# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Interface to GGUF following the logic in ggml.c, in particular
`gguf_init_from_file()`:
https://github.com/ggerganov/llama.cpp/blob/8da46278e1a57107591653275f8e03a281de94f0/ggml.c#L18016
In order to maintain readability of this file and the ability to
cross-reference ggml.c, here types and field names match those in ggml.c as
much as possible.
This changes only casing for `struct`s to adhere to Mojo naming style.

GGUF is designed with the following core principles:
- The entire model is self-contained in one file.
- The format is extensible so that GGUF can be changed maintaining backwards
  compatibility.
- Weights can be mmap'ed.

GGUFKV is a key-value storage for hyperparameters called metadata.

GGUFTensorInfo describe and is used to locate the tensor data.

The model, its tensors, and all of its metadata are serialized as
little endian.

See the GGUF documentation for more details:
https://github.com/ggerganov/ggml/blob/cce2ac9a5d788c3b6bb72a3b3dbde9247d8b85a7/docs/gguf.md.

NB: all types in this file except `GGUFFile` can be passed around by value and
do not own their data.
The `GGUFFile` allocates all the other objects and deallocates them all in its
destructor.
Other types expose a `.destroy()` method simply to facilitate this.
"""

from collections import List, Optional
from memory.unsafe import DTypePointer
from pathlib import Path
from utils import StaticTuple, Variant

from tensor import Tensor, TensorShape

from . import ggml_quants
from .loadable_model import LlamaHParams, LoadableModel


@value
struct GGMLTypeTrait:
    var type_name: String
    var blck_size: Int
    var type_size: Int
    var is_quantized: Bool


@value
@register_passable("trivial")
struct GGMLType:
    """Enum-like struct matching `ggml_type`, the dtype of a tensor."""

    alias GGML_TYPE_F32: Int32 = 0
    alias GGML_TYPE_F16: Int32 = 1
    alias GGML_TYPE_Q4_0: Int32 = 2
    alias GGML_TYPE_Q4_1: Int32 = 3
    # GGML_TYPE_Q4_2 = 4, support has been removed
    # GGML_TYPE_Q4_3 (5) support has been removed
    alias GGML_TYPE_Q5_0: Int32 = 6
    alias GGML_TYPE_Q5_1: Int32 = 7
    alias GGML_TYPE_Q8_0: Int32 = 8
    alias GGML_TYPE_Q8_1: Int32 = 9
    # k-quantizations
    alias GGML_TYPE_Q2_K: Int32 = 10
    alias GGML_TYPE_Q3_K: Int32 = 11
    alias GGML_TYPE_Q4_K: Int32 = 12
    alias GGML_TYPE_Q5_K: Int32 = 13
    alias GGML_TYPE_Q6_K: Int32 = 14
    alias GGML_TYPE_Q8_K: Int32 = 15
    alias GGML_TYPE_I8: Int32 = 16
    alias GGML_TYPE_I16: Int32 = 17
    alias GGML_TYPE_I32: Int32 = 18
    alias GGML_TYPE_COUNT: Int32 = 19  # marks the end of the enum

    var _value: Int32

    fn __is__(self, other: Self) -> Bool:
        """Checks if this `GGMLType` is the same as `other`.

        Args:
            other: `GGMLType` to check equality against.

        Returns `True` if the `GGMLType`s are the same and `False` otherwise.
        """
        return self._value == other._value

    fn __str__(self) -> String:
        if self._value == self.GGML_TYPE_F16:
            return "F16"
        if self._value == self.GGML_TYPE_F32:
            return "F32"
        if self._value == self.GGML_TYPE_COUNT:
            return "COUNT"
        if self._value == self.GGML_TYPE_I16:
            return "I16"
        if self._value == self.GGML_TYPE_I32:
            return "I32"
        if self._value == self.GGML_TYPE_I8:
            return "I8"
        if self._value == self.GGML_TYPE_Q2_K:
            return "Q2_K"
        if self._value == self.GGML_TYPE_Q3_K:
            return "Q3_K"
        if self._value == self.GGML_TYPE_Q4_0:
            return "Q4_0"
        if self._value == self.GGML_TYPE_Q4_1:
            return "Q4_1"
        if self._value == self.GGML_TYPE_Q4_K:
            return "Q4_K"
        if self._value == self.GGML_TYPE_Q5_0:
            return "Q5_0"
        if self._value == self.GGML_TYPE_Q5_1:
            return "Q5_1"
        if self._value == self.GGML_TYPE_Q5_K:
            return "Q5_K"
        if self._value == self.GGML_TYPE_Q6_K:
            return "Q6_K"
        if self._value == self.GGML_TYPE_Q8_0:
            return "Q8_0"
        if self._value == self.GGML_TYPE_Q8_1:
            return "Q8_1"
        if self._value == self.GGML_TYPE_Q8_K:
            return "Q8_K"
        return "<unknown>"

    @always_inline
    fn dtype(self) raises -> DType:
        # Return uint8 for quantized types.
        if (
            self is Self.GGML_TYPE_Q4_0
            or self is Self.GGML_TYPE_Q4_K
            or self is Self.GGML_TYPE_Q6_K
            or self is Self.GGML_TYPE_Q8_0
        ):
            return DType.uint8
        if self is Self.GGML_TYPE_F16:
            return DType.float16
        if self is Self.GGML_TYPE_F32:
            return DType.float32
        if self is Self.GGML_TYPE_I8:
            return DType.int8
        if self is Self.GGML_TYPE_I16:
            return DType.int16
        if self is Self.GGML_TYPE_I32:
            return DType.int32

        raise "GGML type lacks corresponding DType"

    fn type_trait(self) raises -> GGMLTypeTrait:
        if self is Self.GGML_TYPE_Q4_0:
            return GGMLTypeTrait(
                "q4_0",
                blck_size=ggml_quants.BlockQ40.QK4_0,
                type_size=sizeof[ggml_quants.BlockQ40](),
                is_quantized=True,
            )
        if self is Self.GGML_TYPE_Q4_K:
            return GGMLTypeTrait(
                "q4_K",
                blck_size=ggml_quants.QK_K,
                type_size=sizeof[ggml_quants.BlockQ4K](),
                is_quantized=True,
            )
        if self is Self.GGML_TYPE_Q6_K:
            return GGMLTypeTrait(
                "q6_K",
                blck_size=ggml_quants.QK_K,
                type_size=sizeof[ggml_quants.BlockQ6K](),
                is_quantized=True,
            )
        if self is Self.GGML_TYPE_Q8_0:
            return GGMLTypeTrait(
                "q8_0",
                blck_size=ggml_quants.BlockQ80.QK8_0,
                type_size=sizeof[ggml_quants.BlockQ80](),
                is_quantized=True,
            )
        if self is Self.GGML_TYPE_F32:
            return GGMLTypeTrait(
                "f32",
                blck_size=1,
                type_size=sizeof[DType.float32](),
                is_quantized=False,
            )
        if self is Self.GGML_TYPE_F16:
            return GGMLTypeTrait(
                "f16",
                blck_size=1,
                type_size=sizeof[DType.float16](),
                is_quantized=False,
            )

        raise "type trait " + str(self._value) + " not implemented yet"


@value
@register_passable("trivial")
struct GGUFString(Stringable):
    # The length of the string in bytes.
    var n: UInt64
    # The string as a UTF-8 non-null-terminated string.
    # Note: this matches GGUF spec but
    # is unsafe to be used without proper initialization.
    var data: DTypePointer[DType.uint8]

    @always_inline
    fn destroy(owned self):
        self.data.free()

    @always_inline
    fn __str__(self) -> String:
        return str(StringRef(self.data, int(self.n)))


@value
@register_passable("trivial")
struct GGUFArray:
    # The length of the array
    var n: UInt64

    var data: UnsafePointer[GGUFValue]

    # Max amount of items to print for an array.
    alias max_print_size = 5

    fn __init__(inout self, size: Int):
        self.n = size
        self.data = UnsafePointer[GGUFValue].alloc(size)

    @always_inline
    fn destroy(owned self):
        self.data.free()

    @always_inline
    fn __str__(self) -> String:
        var res = String("[ ")
        for i in range(self.n.min(self.max_print_size)):
            res += str(self.data[i])
            res += " "
        res += "]"
        return res


@value
@register_passable("trivial")
struct GGUFType:
    """Enum-like struct matching `gguf_type`, a metadata value type."""

    alias GGUF_TYPE_UINT8: Int32 = 0
    alias GGUF_TYPE_INT8: Int32 = 1
    alias GGUF_TYPE_UINT16: Int32 = 2
    alias GGUF_TYPE_INT16: Int32 = 3
    alias GGUF_TYPE_UINT32: Int32 = 4
    alias GGUF_TYPE_INT32: Int32 = 5
    alias GGUF_TYPE_FLOAT32: Int32 = 6
    alias GGUF_TYPE_BOOL: Int32 = 7
    alias GGUF_TYPE_STRING: Int32 = 8
    alias GGUF_TYPE_ARRAY: Int32 = 9
    alias GGUF_TYPE_UINT64: Int32 = 10
    alias GGUF_TYPE_INT64: Int32 = 11
    alias GGUF_TYPE_FLOAT64: Int32 = 12
    alias GGUF_TYPE_COUNT: Int32 = 13  # Marks the end of the enum.

    var _value: Int32

    fn __is__(self, other: Self) -> Bool:
        """Checks if this `GGUFType` is the same as `other`.

        Args:
            other: `GGUFType` to check equality against.

        Returns `True` if the `GGUFType`s are the same and `False` otherwise.
        """
        return self._value == other._value

    @always_inline
    fn dispatch[
        T: AnyType, func: fn[type: DType] () raises capturing -> T
    ](self) raises -> T:
        if self is Self.GGUF_TYPE_UINT8:
            return func[DType.uint8]()
        if self is Self.GGUF_TYPE_INT8:
            return func[DType.int8]()
        if self is Self.GGUF_TYPE_UINT16:
            return func[DType.uint16]()
        if self is Self.GGUF_TYPE_INT16:
            return func[DType.int16]()
        if self is Self.GGUF_TYPE_UINT32:
            return func[DType.uint32]()
        if self is Self.GGUF_TYPE_INT32:
            return func[DType.int32]()
        if self is Self.GGUF_TYPE_UINT64:
            return func[DType.uint64]()
        if self is Self.GGUF_TYPE_INT64:
            return func[DType.int64]()
        if self is Self.GGUF_TYPE_FLOAT32:
            return func[DType.float32]()
        if self is Self.GGUF_TYPE_FLOAT64:
            return func[DType.float64]()
        if self is Self.GGUF_TYPE_BOOL:
            return func[DType.bool]()

        # GGUF_TYPE_STRING and GGUF_TYPE_ARRAY must be handled separately.
        raise "only GGUF types corresponding to dtypes are supported"


@value
struct GGUFValue:
    alias _type = Variant[
        UInt8,
        Int8,
        UInt16,
        Int16,
        UInt32,
        Int32,
        UInt64,
        Int64,
        Float32,
        Float64,
        Bool,
        GGUFString,
        GGUFArray,
    ]
    var _value: Self._type

    @staticmethod
    fn from_dtype[type: DType](value: Scalar[type]) raises -> Self:
        @parameter
        if type.is_bool():
            # Treat `Bool` as an exception since it isn't `Scalar[DType.bool]`.
            return Self(Bool(value))
        else:
            return Self(value)

    @always_inline
    fn destroy(owned self):
        if self._value.isa[GGUFString]():
            self._value.unsafe_get[GGUFString]()[].destroy()
        elif self._value.isa[GGUFArray]():
            self._value.unsafe_get[GGUFArray]()[].destroy()

    @always_inline
    fn float(self) raises -> Float64:
        if self._value.isa[Float32]():
            return self._value.unsafe_get[Float32]()[].cast[DType.float64]()
        if self._value.isa[Float64]():
            return self._value.unsafe_get[Float64]()[]

        raise "GGUFValue is not a float32 or float64"

    fn __int__(self) raises -> Int:
        if self._value.isa[Int8]():
            return int(self._value.unsafe_get[Int8]()[])
        if self._value.isa[UInt8]():
            return int(self._value.unsafe_get[UInt8]()[])
        if self._value.isa[UInt16]():
            return int(self._value.unsafe_get[UInt16]()[])
        if self._value.isa[Int16]():
            return int(self._value.unsafe_get[Int16]()[])
        if self._value.isa[UInt32]():
            return int(self._value.unsafe_get[UInt32]()[])
        if self._value.isa[Int32]():
            return int(self._value.unsafe_get[Int32]()[])
        if self._value.isa[Int64]():
            return int(self._value.unsafe_get[Int64]()[])
        if self._value.isa[Float32]():
            return int(self._value.unsafe_get[Float32]()[])
        if self._value.isa[Bool]():
            return int(self._value.unsafe_get[Bool]()[])
        if self._value.isa[GGUFString]():
            raise "can't convert GGUFString to int"
        if self._value.isa[GGUFArray]():
            raise "can't convert GGUFArray to int"

        raise "unknown GGUFValue"

    fn __str__(self) -> String:
        if self._value.isa[Int8]():
            return str(self._value.unsafe_get[Int8]()[])
        if self._value.isa[UInt8]():
            return str(self._value.unsafe_get[UInt8]()[])
        if self._value.isa[UInt16]():
            return str(self._value.unsafe_get[UInt16]()[])
        if self._value.isa[Int16]():
            return str(self._value.unsafe_get[Int16]()[])
        if self._value.isa[UInt32]():
            return str(self._value.unsafe_get[UInt32]()[])
        if self._value.isa[Int32]():
            return str(self._value.unsafe_get[Int32]()[])
        if self._value.isa[Int64]():
            return str(self._value.unsafe_get[Int64]()[])
        if self._value.isa[Float32]():
            return str(self._value.unsafe_get[Float32]()[])
        if self._value.isa[Bool]():
            return str(self._value.unsafe_get[Bool]()[])
        if self._value.isa[GGUFString]():
            return str(self._value.unsafe_get[GGUFString]()[])
        if self._value.isa[GGUFArray]():
            return str(self._value.unsafe_get[GGUFArray]()[])
        return "<unknown>"


@value
struct GGUFKV:
    # The key of the metadata. It is a standard GGUF string, with the following caveats:
    # - It must be a valid ASCII string.
    # - It must be a hierarchical key, where each segment is `lower_snake_case`
    #   and separated by a `.`.
    # - It must be at most 2^16-1/65535 bytes long.
    # Any keys that do not follow these rules are invalid.
    var key: GGUFString
    var value: GGUFValue

    @always_inline
    fn destroy(owned self):
        self.key.destroy()
        self.value.destroy()

    fn __str__(self) -> String:
        return str(self.key) + ": " + str(self.value)


@value
@register_passable("trivial")
struct GGUFHeader:
    # Magic number to announce that this is a GGUF file.
    # Must be `GGUF` at the byte level: `0x47` `0x47` `0x55` `0x46`.
    # Your executor might do little-endian byte order, so it might be
    # checking for 0x46554747 and letting the endianness cancel out.
    # Consider being *very* explicit about the byte order here.
    var magic: StaticTuple[UInt8, 4]
    # The version of the format implemented.
    # Must be `3` for version described in this spec, which introduces big-endian support.
    #
    # This version should only be increased for structural changes to the format.
    # Changes that do not affect the structure of the file should instead
    # update the metadata to signify the change.
    var version: UInt32
    # This number of tensors in the file is explicit, instead of being included
    # in the metadata, to ensure it is always present for loading the tensors.
    var n_tensors: UInt64
    # The number of metadata key-value pairs.
    var n_kv: UInt64

    fn __init__(inout self):
        self.magic = StaticTuple[UInt8, 4](
            ord("N"), ord("U"), ord("L"), ord("L")
        )
        self.version = 0
        self.n_tensors = 0
        self.n_kv = 0

    fn __str__(self) -> String:
        return (
            "version: "
            + str(self.version)
            + "\nn_tensors: "
            + str(self.n_tensors)
            + "\nn_kv: "
            + str(self.n_kv)
        )


@value
@register_passable("trivial")
struct GGUFTensorInfo:
    alias GGML_MAX_DIMS: Int = 4

    # The name of the tensor. It is a standard GGUF string, with the caveat
    # that it must be at most 64 bytes long.
    var name: GGUFString
    # The number of dimensions in the tensor.
    # Currently at most 4, but this may change in the future.
    var n_dims: UInt32
    var ne: StaticTuple[UInt64, Self.GGML_MAX_DIMS]

    var type: GGMLType

    # The offset of the tensor's data in this file in bytes.
    #
    # This offset is relative to `tensor_data`, not to the start
    # of the file, to make it easier for writers to write the file.
    # Readers should consider exposing this offset relative to the
    # file to make it easier to read the data.
    #
    # Must be a multiple of `ALIGNMENT`.
    var offset: UInt64

    # For writing API.
    var data: DTypePointer[DType.invalid]
    var size: Int

    # Padding for aligning columns when printing tensor info
    alias name_padding = 32
    alias type_padding = 8

    fn __str__(self) -> String:
        # Create padding to line up tensor name, type, and dims.
        var res = str(self.name)
        var sep = self.name_padding - len(res)
        if sep < 1:
            sep = 1
        var spaces = String()
        for _ in range(sep):
            spaces += " "

        var type_str = str(self.type)
        var type_sep = self.type_padding - len(type_str)
        if type_sep < 1:
            type_sep = 1
        var type_spaces = String()
        for _ in range(type_sep):
            type_spaces += " "

        res += spaces + str(self.type) + type_spaces + "[ "
        for i in range(self.n_dims):
            res += str(self.ne[i])
            res += " "
        res += "]"
        return res

    @always_inline
    fn destroy(owned self):
        self.name.destroy()
        self.data.free()

    @always_inline
    fn num_bytes(self) raises -> Int:
        var ne = self.ne
        var num_elements = ne[0] * ne[1] * ne[2] * ne[3]

        var type_trait = self.type.type_trait()
        return int(num_elements * type_trait.type_size // type_trait.blck_size)

    fn tensor_dims(self) -> List[Int]:
        """Converts from GGML `ne` to dims compatible with stdlib `Tensor`.

        Returns:
            A `List` of dims compatible with stdlib `TensorShape`.
        """
        var n_dims = int(self.n_dims)
        var dims = List[Int](capacity=n_dims)
        for i in range(n_dims):
            # Opposite to `TensorSpec`, GGUF stores the inner dimension at
            # the smaller index, so reverse them.
            dims.append(int(self.ne[n_dims - i - 1]))

        return dims

    @always_inline
    fn storage_tensor_shape(self) raises -> TensorShape:
        """Computes the `TensorShape` for the storage backing this tensor.

        Returns:
            A `TensorShape` describing this GGUF tensor's torage.
        """
        var dims = self.tensor_dims()
        if self.type.type_trait().is_quantized:
            if len(dims) != 2:
                raise (
                    "GGML to stdlib tensor only supports quantized matrices"
                    " currently but got tensor of rank: "
                    + str(len(dims))
                )

            # TODO(#31206): Support more principled compatibility between:
            # - Custom quantized types such as in ggml-quants.h.
            # - Mojo types.
            # - MO types.
            return TensorShape(dims[0], self.num_bytes() // dims[0])

        return TensorShape(dims)


struct GGUFReader:
    var offset: Int
    var f: FileHandle

    fn __init__(inout self, owned f: FileHandle):
        self.offset = 0
        self.f = f^

    @always_inline
    fn align_to(inout self, alignment: Int) raises -> None:
        var overshoot = self.offset % alignment
        if overshoot == 0:
            return

        self.seek(alignment - overshoot)

    @always_inline
    fn read_bytes(inout self, num_bytes: Int) raises -> Tensor[DType.uint8]:
        self.offset += num_bytes
        return self.f.read_bytes(num_bytes)

    @always_inline
    fn seek(inout self, num_bytes: Int) raises:
        self.offset += num_bytes
        _ = self.f.seek(num_bytes)

    @always_inline
    fn dtype_element[type: DType](inout self) raises -> Scalar[type]:
        var bytes_tensor: Tensor[DType.uint8] = self.read_bytes(
            sizeof[Scalar[type]]()
        )
        var result = bytes_tensor.unsafe_ptr().bitcast[type]().load()
        _ = bytes_tensor^

        return result

    @always_inline
    fn gguf_string(inout self) raises -> GGUFString:
        var n = int(self.dtype_element[DType.uint64]())
        var key_data: Tensor[DType.uint8] = self.read_bytes(n)
        return GGUFString(n, key_data._steal_ptr().bitcast[DType.uint8]())

    fn gguf_kv(inout self) raises -> GGUFKV:
        @always_inline
        @parameter
        fn _gguf_value[type: DType]() raises -> GGUFValue:
            var bytes_tensor: Tensor[DType.uint8] = self.read_bytes(
                sizeof[Scalar[type]]()
            )
            var result = bytes_tensor.unsafe_ptr().bitcast[type]().load()
            _ = bytes_tensor^

            return GGUFValue.from_dtype[type](result)

        @always_inline
        @parameter
        fn _sizeof[type: DType]() raises -> Int:
            return sizeof[type]()

        var key = self.gguf_string()
        if (
            StringRef(key.data.bitcast[DType.uint8](), int(key.n))
            == "general.alignment"
        ):
            raise "don't support specifying alignment"

        var kv_type = GGUFType(self.dtype_element[DType.int32]())
        if kv_type is GGUFType.GGUF_TYPE_STRING:
            return GGUFKV(key, GGUFValue(self.gguf_string()))

        if kv_type is GGUFType.GGUF_TYPE_ARRAY:
            var array_type = GGUFType(self.dtype_element[DType.int32]())
            if array_type is GGUFType.GGUF_TYPE_ARRAY:
                raise (
                    "Array of arrays not supported yet. Please raise an issue"
                )

            var array_n = int(self.dtype_element[DType.uint64]())
            # Note: the underlying `array.data` ptr is uninitialized
            var array = GGUFArray(array_n)
            if array_type is GGUFType.GGUF_TYPE_STRING:
                for i in range(array_n):
                    initialize_pointee_copy(
                        array.data + i, GGUFValue(self.gguf_string())
                    )

                return GGUFKV(key, GGUFValue(array))

            for i in range(array_n):
                initialize_pointee_copy(
                    array.data + i,
                    array_type.dispatch[GGUFValue, _gguf_value](),
                )

            return GGUFKV(key, GGUFValue(array))

        # Dispatch on dtype.
        return GGUFKV(key, kv_type.dispatch[GGUFValue, _gguf_value]())

    fn gguf_tensor_info(inout self) raises -> GGUFTensorInfo:
        var name = self.gguf_string()
        var n_dims = self.dtype_element[DType.uint32]()

        var ne = StaticTuple[UInt64, GGUFTensorInfo.GGML_MAX_DIMS]()
        for i in range(GGUFTensorInfo.GGML_MAX_DIMS):
            ne[i] = 1

        for i in range(int(n_dims)):
            ne[i] = self.dtype_element[DType.uint64]()

        var type = GGMLType(self.dtype_element[DType.int32]())
        var offset = self.dtype_element[DType.uint64]()

        return GGUFTensorInfo(
            name,
            n_dims,
            ne,
            type,
            offset,
            data=DTypePointer[DType.invalid](),
            size=0,
        )


struct GGUFFile(LoadableModel):
    """A container for all metadata describing the weights in a GGUF file."""

    # This is called GGUFFile to match `gguf_file_t` in gguf.md, but note that
    # this matches `gguf_context` in ggml.c.

    # This context owns all memory of its fields and their fields,
    # transitively.
    # All GGUF types with non-trivial memory management should implement a
    # `.destroy()` method, which is called in `GGUFFile`'s destructor.

    alias GGUF_DEFAULT_ALIGNMENT = 32
    alias GGUF_MAGIC = "GGUF"

    var header: GGUFHeader

    var kv: List[GGUFKV]
    var infos: List[GGUFTensorInfo]

    var alignment: Int

    # The offset of the tensor data in the file.
    # `GGUFTensorInfo.offset` is relative to this.
    var offset: Int
    # Size of the tensor data section in bytes.
    var size: Int

    # The open GGUF model file.
    var fp: FileHandle

    fn __init__(inout self, model_path: Path) raises:
        var reader = GGUFReader(open(model_path, "r"))

        # Read the header.
        var magic: Tensor[DType.uint8] = reader.read_bytes(
            sizeof[StaticTuple[Int8, 4]]()
        )
        for i in range(magic.num_elements()):
            if magic[i] != GGUFFile.GGUF_MAGIC.as_uint8_ptr().load(i):
                raise "invalid magic character"

        var version = reader.dtype_element[DType.uint32]()
        if version == 1:
            raise "GGUFv1 is not supported"

        var n_tensors = int(reader.dtype_element[DType.uint64]())
        var n_kv = int(reader.dtype_element[DType.uint64]())

        self.header = GGUFHeader(
            StaticTuple[UInt8, 4](magic[0], magic[1], magic[2], magic[3]),
            version,
            n_tensors,
            n_kv,
        )

        # Read the kv pairs.
        self.kv = List[GGUFKV](capacity=n_kv)
        for _ in range(n_kv):
            self.kv.append(reader.gguf_kv())

        # Read the tensor infos.
        self.infos = List[GGUFTensorInfo](capacity=n_tensors)
        for _ in range(n_tensors):
            self.infos.append(reader.gguf_tensor_info())

        self.alignment = GGUFFile.GGUF_DEFAULT_ALIGNMENT
        # TODO: Set alignment from general.alignment key.

        reader.align_to(self.alignment)
        self.offset = reader.offset

        # Compute total size of the data section accounting for alignment.
        self.size = 0
        for i in range(n_tensors):
            var size_cur = self.infos[i].num_bytes()

            @always_inline
            fn pad(x: Int, n: Int) -> Int:
                return (x + n - 1) & ~(n - 1)

            self.size += pad(size_cur, self.alignment)

        self.fp = open(model_path, "r")

    fn __moveinit__(inout self, owned other: GGUFFile):
        @always_inline
        fn exchange[T: Movable](inout old_var: T, owned new_value: T) -> T:
            var old_value = old_var^
            old_var = new_value^
            return old_value^

        self.header = other.header

        self.kv = exchange(other.kv, List[GGUFKV]())
        self.infos = exchange(other.infos, List[GGUFTensorInfo]())

        self.alignment = other.alignment
        self.offset = other.offset
        self.size = other.size

        self.fp = other.fp^

    fn __del__(owned self):
        for i in range(int(self.header.n_kv)):
            self.kv[i].destroy()

        for i in range(self.n_tensors()):
            self.infos[i].destroy()

    fn __str__(self) -> String:
        var res = str(self.header) + "\n"
        for i in range(self.header.n_kv):
            res += str(self.kv[i])
            res += "\n"
        for i in range(self.header.n_tensors):
            res += str(self.infos[i])
            res += "\n"
        return res

    fn __getitem__(self, key: String) raises -> GGUFValue:
        """Returns the metadata value for key, raising if not found."""
        for kv in self.kv:
            if str(kv[].key) == key:
                return kv[].value

        raise "GGUF key: " + key + " not found"

    fn get[
        type: DType
    ](
        inout self, key: String, layer_idx: Optional[Int] = None
    ) raises -> Tensor[type]:
        var full_key = key + ".weight"
        if layer_idx:
            full_key = "blk." + str(layer_idx.value()[]) + "." + full_key

        for i in range(self.n_tensors()):
            var info = self.infos[i]
            if str(info.name) != full_key:
                continue

            if type != info.type.dtype():
                raise "compile/runtime dtype mismatch of " + str(
                    type
                ) + "; expected " + str(info.type.dtype()) + " for " + str(
                    info.name
                )

            # Add tensor data offset since `info.offset` is from the start of
            # the tensor data.
            _ = self.fp.seek(self.offset + int(info.offset))
            var bytes_tensor = Tensor[DType.uint8](
                self.fp.read_bytes(info.num_bytes())
            )

            return Tensor(
                info.storage_tensor_shape(),
                bytes_tensor._steal_ptr().bitcast[type](),
            )

        raise "key not found"

    fn ggml_type(
        self, key: String, layer_idx: Optional[Int] = None
    ) raises -> GGMLType:
        """Reads the GGML type for the specified tensor."""
        var full_key = key + ".weight"
        if layer_idx:
            full_key = "blk." + str(layer_idx.value()[]) + "." + full_key

        for i in range(self.n_tensors()):
            var info = self.infos[i]
            if str(info.name) != full_key:
                continue

            return info.type

        raise "key: " + key + " not found in ggml_type"

    fn hyperparams(self) raises -> LlamaHParams:
        """Loads Llama hyperparameters by reading the GGUF metadata."""
        var dims = int(self["llama.embedding_length"])
        var n_heads = int(self["llama.attention.head_count"])
        var n_kv_heads = int(self["llama.attention.head_count_kv"])

        # Compute the vocab size from the token embedding shape.
        # This works around -1 vocab_size from Llama 1 and 2 conversion.
        # See for example: https://github.com/ggerganov/llama.cpp/pull/4258.
        var vocab_size: Optional[Int] = None
        for info in self.infos:
            if str(info[].name) != "token_embd.weight":
                continue

            # Take the 2nd dim since token_embd is [dims, vocab_size].
            vocab_size = int(info[].ne[1])

        if not vocab_size:
            # As a last resort try the vocab_size metadata field.
            vocab_size = int(self["llama.vocab_size"])

        return LlamaHParams(
            dims=dims,
            n_layers=int(self["llama.block_count"]),
            n_heads=n_heads,
            norm_eps=self["llama.attention.layer_norm_rms_epsilon"].float(),
            n_kv_heads=n_kv_heads,
            vocab_size=vocab_size.value()[],
            head_dim=dims // n_heads,
            n_rep=n_heads // n_kv_heads,
        )

    @always_inline
    fn n_tensors(self) -> Int:
        return int(self.header.n_tensors)
