# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""The body of the Llama 3 model definition."""

from collections import List, Optional
from pathlib import Path

from max.graph import ops, Dim, Graph, TensorType, Type, Symbol
from max.graph.quantization import (
    Float32Encoding,
    Q4_0Encoding,
    Q4_KEncoding,
    Q6_KEncoding,
    QuantizationEncoding,
)
from pipelines.nn import (
    Attention,
    Embedding,
    RMSNorm,
    FeedForward,
    Transformer,
    TransformerBlock,
)
from pipelines.weights.gguf import GGMLType, GGUFFile
from pipelines.weights.loadable_model import LlamaHParams


def encoding_id_from_ggml_type(ggml_type: GGMLType) -> String:
    """Returns the quantization encoding id for a given GGML tensor type."""
    if ggml_type is GGMLType.GGML_TYPE_F32:
        return Float32Encoding.id()
    elif ggml_type is GGMLType.GGML_TYPE_Q4_0:
        return Q4_0Encoding.id()
    elif ggml_type is GGMLType.GGML_TYPE_Q4_K:
        return Q4_KEncoding.id()
    elif ggml_type is GGMLType.GGML_TYPE_Q6_K:
        return Q6_KEncoding.id()

    raise "unknown GGML type:" + str(ggml_type)


struct Llama3[encoding: QuantizationEncoding = Float32Encoding]:
    alias batch_size = 1

    var model: GGUFFile

    def __init__(inout self, model_path: Path):
        self.model = GGUFFile(model_path)

    def build_graph(inout self, name: String) -> Graph:
        params = self.hyperparams()
        cache_type = TensorType(
            DType.float32,
            Dim.dynamic(),
            params.n_layers,
            Self.batch_size,
            params.n_kv_heads,
            params.head_dim,
        )
        tokens_type = TensorType(DType.int64, self.batch_size, Dim.dynamic())
        g = Graph(name, List[Type](tokens_type, cache_type, cache_type))

        @parameter
        def weight[
            weight_type: QuantizationEncoding = encoding
        ](
            name: String,
            layer: Optional[Int] = None,
            # Torch models store weights in a transposed format, and transpose again at use.
            # The q4_0 format transposes the weights compared with the fp32 formats.
            transpose: Bool = (weight_type.id() == Float32Encoding.id()),
        ) -> (Symbol, String):
            """Stages a constant op according to the quantization encoding."""

            # Set uint8 dtype if quantized and float32 otherwise, since all
            # quantization encodings currently have uint8 dtype.
            alias dtype = DType.float32 if weight_type.id() == Float32Encoding.id() else DType.uint8

            # Also return the quantization encoding for this constant.
            encoding_id = encoding_id_from_ggml_type(
                self.model.ggml_type(name, layer)
            )

            weight = g.constant(self.model.get[dtype](name, layer))
            if transpose:
                weight = weight.swapaxes(-1, -2)

            return weight, encoding_id

        def norm(name: String, layer: Optional[Int] = None) -> RMSNorm:
            # GGUF always stores these as float32
            w, _ = weight[Float32Encoding](name, layer, transpose=False)
            return RMSNorm(params.norm_eps, w)

        layers = List[TransformerBlock]()
        for layer in range(params.n_layers):
            attention = Attention(
                n_heads=params.n_heads,
                n_kv_heads=params.n_kv_heads,
                head_dim=params.head_dim,
                dim=params.dims,
                enable_custom_rope_kernel=False,
                use_custom_attention=True,
                wq=weight("attn_q", layer),
                wk=weight("attn_k", layer),
                wv=weight("attn_v", layer),
                wo=weight("attn_output", layer),
            )
            var feed_forward = FeedForward(
                w1=weight("ffn_gate", layer),
                w2=weight("ffn_down", layer),
                w3=weight("ffn_up", layer),
            )

            layers.append(
                TransformerBlock(
                    attention=attention,
                    feed_forward=feed_forward,
                    attention_norm=norm("attn_norm", layer),
                    ffn_norm=norm("ffn_norm", layer),
                )
            )

        embedding = Embedding(weight("token_embd", transpose=False))
        model = Transformer(
            dim=params.dims,
            n_heads=params.n_heads,
            embedding=embedding,
            layers=layers,
            norm=norm("output_norm"),
            output=weight("output"),
            theta=500000.0,
        )

        # outputs (logits, k_cache, v_cache)
        outputs = model(tokens=g[0], k_cache=g[1], v_cache=g[2])
        logits = outputs[0]
        g.output(List(logits[-1, axis=1], outputs[1], outputs[2]))
        return g

    def hyperparams(self) -> LlamaHParams:
        """Returns hyperparameters corresponding to this Llama 3 model."""
        return self.model.hyperparams()
