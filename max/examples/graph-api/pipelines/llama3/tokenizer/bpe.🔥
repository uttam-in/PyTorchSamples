# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""A byte pair encoding tokenizer implementation for use with LLMs."""

from base64 import b64decode
from collections import Dict, List, Optional
from pathlib import Path

from tensor import Tensor

from .arena_linked_list import ArenaLinkedList
from .max_heap import MinHeap, OrderedElement


# TODO(MSTDL-590): upstream split/lines into stdlib
@value
struct _SplitIter[is_mutable: Bool, lifetime: AnyLifetime[is_mutable].type]:
    """Iterator over splits of a string. Returns string slices, preventing
    allocating a new String for each slice of the original string, and
    allows specifing a maximum number of splits."""

    var span: Span[UInt8, is_mutable, lifetime]
    var split: String
    var max: Optional[Int]
    var done: Bool

    fn __iter__(self) -> Self:
        return self

    fn __next__(inout self) -> StringSlice[is_mutable, lifetime]:
        """Return the next split slice via StringRef.find()."""
        # If we've already split `max` times, return the final slice.
        if self.max and self.max.value()[] == 0:
            self.done = True
            return StringSlice(unsafe_from_utf8=self.span)

        var next_newline: Int
        # If we've already split `max` times, return the final slice.
        var split = self.split._strref_dangerous()
        var slice = StringSlice(unsafe_from_utf8=self.span)
        if (next_newline := slice._strref_dangerous().find(split)) != -1:
            var line = self.span[:next_newline]
            # advance the span start until the end of the found match
            self.span = self.span[next_newline + len(self.split) :]
            if self.max:
                self.max = self.max.value()[] - 1
            return StringSlice(unsafe_from_utf8=line)
        self.done = True
        return StringSlice(unsafe_from_utf8=self.span)

    fn __len__(inout self) -> Int:
        # Return a number >0 unless max split is -1 (signals end of iteration)
        # or span is empty.
        return 0 if self.done else 1


def split(
    string: String, split: String, /, max: Int = -2
) -> _SplitIter[False, __lifetime_of(string)]:
    """Iterate over string slice references. Splits at most `max` times."""
    return _SplitIter(string.as_bytes_slice(), split, max, False)


def lines(string: String) -> _SplitIter[False, __lifetime_of(string)]:
    return split(string, "\n")


@value
struct Token(CollectionElement):
    """A token-score pair for storing a BPE vocabulary."""

    var token: String
    var score: Float32

    fn __repr__(self) -> String:
        return (
            str("Token(")
            + repr(self.token)
            + ", score="
            + str(self.score)
            + ")"
        )


@value
struct TokenWithID(CollectionElement):
    """A string token, along with its ID in the vocabulary (or 0 for unk)."""

    var token: String
    var id: Int

    fn __repr__(self) -> String:
        return "Token(" + self.token.__repr__() + ", " + str(self.id) + ")"


@value
struct MergeOption(OrderedElement):
    """Metadata for tracking possible BPE merges in a priority queue."""

    var left: ArenaLinkedList[String].ID
    var right: ArenaLinkedList[String].ID
    var score: Float32
    var checksum: Int

    fn __lt__(self, other: Self) -> Bool:
        # 1. Lower scores are better
        # 2. If two options with the same score, the earlier one is better
        return (self.score < other.score) or (
            self.score == other.score and self.left < other.left
        )

    fn __repr__(self) -> String:
        return (
            str("MergeOption(left=")
            + str(self.left)
            + ", right="
            + str(self.right)
            + ", score="
            + str(self.score)
            + ", checksum="
            + str(self.checksum)
            + ")"
        )


def read[T: CollectionElement](inout span: Span[UInt8, _, _]) -> T:
    """Read a binary type out of a byte buffer and increment the pointer."""
    value = span.unsafe_ptr().bitcast[T]()[]
    span = span[sizeof[T]() :]
    return value^


@value
struct BPETokenizer:
    """A Byte Pair Encoding string tokenizer.

    [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)
    can tokenize strings of any language or encoding based on a learned input
    dictionary. This implementation may not be fully featured, but is fast
    and compatible with some major LLMs such as Llama. It uses the same
    basic approach as sentencepiece, with an implementation with good
    algorithmic performance, but not fully optimized.
    """

    var vocab: List[Token]
    var token_ids: Dict[String, Int]

    def __init__(inout self):
        self.vocab = List[Token]()
        self.token_ids = Dict[String, Int]()

    @staticmethod
    def from_tiktoken(data: String) -> BPETokenizer:
        """Construct a BPETokenizer instance given binary token scores.

        The file must have the following binary format:
        ```
        [max_token_len:Int32]
        [score_0:Float32]
        [token_len_0:Int32]
        [token_0:(UInt8*token_len_0)]
        [score_1:Float32]
        [token_len_1:Int32]
        [token_1:(UInt8*token_len_0)]
        ...
        ```
        """
        model = BPETokenizer()

        for line in lines(data):
            if not len(line.as_bytes_slice()):
                continue
            splt = split(line, " ", 1)
            token = b64decode(splt.__next__())
            rank = int(splt.__next__())  # always exactly lineno as well
            model.add_token(token, rank)

        return model^

    @staticmethod
    def from_tiktoken(path: Path) -> Self:
        """Construct a BPETokenizer instance given binary token scores.

        See `BPETokenizer.from_bytes()` for the expected file format.
        """
        with open(path, "r") as file:
            return Self.from_tiktoken(file.read())

    @staticmethod
    def from_binary(data: String) -> BPETokenizer:
        """Construct a BPETokenizer instance given binary token scores.

        The file must have the following binary format:
        ```
        [max_token_len:Int32]
        [score_0:Float32]
        [token_len_0:Int32]
        [token_0:(UInt8*token_len_0)]
        [score_1:Float32]
        [token_len_1:Int32]
        [token_1:(UInt8*token_len_0)]
        ...
        ```
        """

        var model = BPETokenizer()
        span = data.as_bytes_slice()

        _max_token_len = read[Int32](span)
        while len(span):
            score = read[Float32](span)
            token_len = int(read[Int32](span))
            token = String(span[: token_len + 1])
            span = span[token_len:]
            model.add_token(token, score)

        return model^

    @staticmethod
    def from_binary(path: Path) -> Self:
        """Construct a BPETokenizer instance given binary token scores.

        See `BPETokenizer.from_bytes()` for the expected file format.
        """
        with open(path, "r") as file:
            return Self.from_binary(file.read())

    def add_token(inout self, token: String, score: Float32):
        """Add a token to the vocabulary."""
        if token not in self.token_ids:
            self.token_ids[token] = len(self.vocab)
        self.vocab.append(Token(token, score))

    def encode(
        self,
        str: String,
        bos: Optional[String] = None,
        eos: Optional[String] = None,
    ) -> List[TokenWithID]:
        """Encode a string according to the BPE algorithm.

        The BPE vocabulary is a set of scored strings. BPE starts by
        considering every character in the input string as its own token,
        and then greedily merges the highest scoring adjacent pair
        until no more adjacent token merges exist in the vocabulary.

        We implement the tokens as a linked list, with a priority queue
        of merge options. We execute the highest-scoring merge, adding
        new merge options to the priority queue if they exist in the vocabulary.
        We can't remove out-dated merge options from the priority queue, so
        instead we add a checksum to them, which is the length of the merge
        they're expecting. Linked list elements only stop existing or grow
        in length, so we can always safely recognize an outdated merge.
        """
        output = List[TokenWithID]()
        if bos and bos.value()[] in self.token_ids:
            output.append(
                TokenWithID(bos.value()[], self.token_ids[bos.value()[]])
            )

        merge_options = MinHeap[MergeOption]()
        tokens = ArenaLinkedList[String]()

        @parameter
        def maybe_add_merge(left: tokens.ID, right: tokens.ID):
            merged = tokens[left] + tokens[right]
            if token_id := self.token_ids.find(merged):
                score = self.vocab[token_id.value()[]].score
                merge_options.push(MergeOption(left, right, score, len(merged)))

        # Initialize the tokens linked-list and initial merges.
        var prev: Optional[ArenaLinkedList[String].ID] = None
        for i in range(len(str)):
            id = tokens.append(str[i])
            if prev:
                maybe_add_merge(prev.value()[], id)
            prev = id

        while merge_options:
            merge = merge_options.pop()
            # Check whether the best merge is still valid
            if merge.left not in tokens or merge.right not in tokens:
                continue  # outdated merge option
            merged = tokens[merge.left] + tokens[merge.right]
            # TODO: don't create merged token before checking
            if len(merged) != merge.checksum:
                continue  # outdated merge option
            # Merge the right token into the left token, then
            # add any new valid merge options to the priority queue.
            left = tokens.prev(merge.left)
            right = tokens.next(merge.right)
            tokens[merge.left] = merged
            tokens.remove(merge.right)
            if right:
                maybe_add_merge(merge.left, right.value()[])
            if left:
                maybe_add_merge(left.value()[], merge.left)

        # Loop through the final list and construct the token sequence.
        node_id = tokens._head
        while node_id:
            id = node_id.value()[]
            token = tokens[id]
            output.append(TokenWithID(token, self._encode_token(token)))
            node_id = tokens.next(id)

        if eos and eos.value()[] in self.token_ids:
            output.append(
                TokenWithID(eos.value()[], self.token_ids[eos.value()[]])
            )

        return output

    def _encode_token(self, token: String) -> Int:
        return self.token_ids.find(token).or_else(0)
