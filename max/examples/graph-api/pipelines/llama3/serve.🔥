# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

import sys
from collections import List, Optional
from pathlib import cwd, Path
from python.python import _get_global_python_itf, Python, CPython
from runtime.llcl import Runtime, TaskGroup
from sys.ffi import DLHandle
from utils.index import Index
from time import sleep

from max.engine import InferenceSession, Model, SessionOptions, TensorMap
from max.engine._context import _Device
from max.engine._utils import handle_from_config, call_dylib_func
from max.graph import Graph
from max.graph.quantization import (
    Float32Encoding,
    QuantizationEncoding,
    Q4_0Encoding,
    Q4_KEncoding,
    Q6_KEncoding,
)
from max.serve.http.runtime import PythonEntry
from max.serve.server import InferenceServer
from max.serve.service import (
    InferenceRequest,
    InferenceResponse,
    InferenceService,
)
from max.serve._serve_rt import (
    CServerAsync,
    InferenceRequestImpl,
    InferenceResponseImpl,
)

from tensor import Tensor, TensorShape, TensorSpec

from .kv_cache import KVCache
from .model.llama import Llama3
from .tokenizer.tiktoken import TikTokenEncoder
from .tokenizer.regex import set_locale_unicode
from ..llama2.token_sampler.weighted_sampler import WeightedSampler
from ..weights.gguf import GGUFArray, GGUFFile
from ..weights.loadable_model import LlamaHParams, LoadableModel

from .run import (
    Config,
    llama_main,
    cached_weights,
    compile_graph,
    execute,
)


struct Llama3InferenceService[
    EncodingT: QuantizationEncoding = Float32Encoding,
](InferenceService):
    """Inference service for Llama3."""

    var _config: Config
    var _tokenizer: TikTokenEncoder
    var _session: InferenceSession

    var _model: Llama3[EncodingT]
    var _graph: Graph
    var _compiled_model: Model

    var _lib: DLHandle
    var _server_ptr: DTypePointer[DType.invalid]
    var _json_module: PythonObject

    fn __init__(
        inout self,
        owned config: Config,
        owned server_ptr: DTypePointer[DType.invalid],  # TODO: Remove
        owned session: InferenceSession,
    ) raises:
        self._config = config^
        self._server_ptr = server_ptr
        self._session = session^
        self._lib = handle_from_config("serving", ".serve_lib")
        self._json_module = Python.import_module("json")

        print("Building model...")
        self._model = Llama3[EncodingT](self._config.model_path)
        self._graph = self._model.build_graph("llama_model")
        self._compiled_model = self._session.load(
            self._graph, custom_ops_paths=self._config.custom_ops_paths
        )
        self._tokenizer = TikTokenEncoder.cl100k_base_llama3(
            self._model.model["tokenizer.ggml.tokens"]._value.unsafe_get[
                GGUFArray
            ]()[]
        )

    fn __del__(owned self):
        _ = self._config^
        _ = self._tokenizer^
        _ = self._session^
        _ = self._model^
        _ = self._graph^
        _ = self._compiled_model^

    fn init(self, inout server: InferenceServer) raises:
        server._impl.init(self._compiled_model)

    fn infer[
        req_type: InferenceRequest, resp_type: InferenceResponse
    ](inout self, request: req_type, inout response: resp_type) raises -> None:
        var respOr = Variant[resp_type, Error](response^)
        var rt = Runtime()
        rt.run(self.async_infer(request, respOr))
        if respOr.isa[Error]():
            raise respOr.unsafe_take[Error]()
        else:
            response = respOr.unsafe_take[resp_type]()

    fn handle_openai[
        handle_type: fn (PythonEntry) capturing raises -> None,
        req_type: InferenceRequest,
    ](self, request: req_type) raises:
        var api_type = request.get_api_type()
        var payload_type = request.get_payload_type()
        if api_type == 1:
            # OpenAI
            if payload_type == 0:
                # gRPC
                raise Error(
                    "OpenAI API compatibility is only supported via HTTP."
                )
            else:
                # HTTP
                var entry = PythonEntry()
                call_dylib_func[NoneType](
                    self._lib,
                    "M_OpenAIInferenceRequest_fillEntry",
                    self._server_ptr,
                    request.get_ptr(),
                    UnsafePointer.address_of(entry),
                )
                handle_type(entry)

    async fn async_infer[
        req_type: InferenceRequest, resp_type: InferenceResponse
    ](
        inout self, request: req_type, inout response: Variant[resp_type, Error]
    ) -> None:
        @parameter
        def handle(entry: PythonEntry) -> None:
            cpython = _get_global_python_itf().cpython()
            state = cpython.PyGILState_Ensure()

            body = PythonObject(entry.request)
            cpython.Py_IncRef(entry.request)
            stream = False
            if body.__contains__("stream") and body["stream"]:
                stream = True

            # Tokenize prompt and message contents.
            var raw_prompt: String = ""
            for node in body["messages"]:
                raw_prompt += (
                    str(node["role"]) + ":" + str(node["content"]) + "\n"
                )

            prompt = self._tokenizer.encode(raw_prompt)
            sampler = WeightedSampler(
                self._config.temperature, self._config.min_p
            )
            tokens = Tensor[DType.int64](TensorShape(1, len(prompt)))
            for i in range(len(prompt)):
                tokens[Index(0, i)] = prompt[i].id

            outputs = List[String]()
            kv_cache = KVCache(
                self._model.model.hyperparams(),
                self._config.max_tokens,
                self._config.batch_size,
            )

            parent = PythonObject(entry.handler).parent
            cpython.Py_IncRef(entry.handler)

            resp = PythonObject(entry.response)
            cpython.Py_IncRef(entry.response)
            if stream:
                parent.send_response(200)
                parent.send_header("Content-type", "text/event-stream")
                parent.end_headers()

            # The first iteration caches the entire prompt and all subsequent
            # iterations generate one token.
            # Avoid overrunning the cache by setting the trip count accordingly.
            for _ in range(prompt.size, self._config.max_tokens + 1):
                logits = execute(
                    self._session, self._compiled_model, tokens, kv_cache
                )
                token = sampler.sample(logits).selected
                tokens = Tensor(TensorShape(1, 1), Int64(token))

                # if self._tokenizer.is_end_of_text(
                #     tokens[Index(0, tokens.shape()[1] - 1)]
                # ):
                #     break

                next_token = self._tokenizer.decode(token).token
                if not stream:
                    outputs.append(next_token)
                else:
                    # Write chunk response if streaming.
                    var chunk = Python.dict()
                    var choices = Python.list()
                    var choice = Python.dict()
                    var delta = Python.dict()
                    delta["content"] = next_token
                    choice["delta"] = delta
                    choices.append(choice)
                    chunk["choices"] = choices

                    var json_str = self._json_module.dumps(chunk).encode(
                        encoding="utf_8"
                    )
                    try:
                        parent.wfile.write(json_str)
                        parent.wfile.flush()
                    except BrokenPipeError:
                        break

            # Write complete response if not streaming.
            if not stream:
                var raw_message: String = ""
                for output in outputs:
                    raw_message += output[]

                choice = Python.dict()
                message = Python.dict()
                message["role"] = "assistant"
                message["content"] = raw_message
                choice["index"] = 0
                choice["message"] = message

                choices = Python.list()
                choices.append(choice)
                resp["choices"] = choices

                parent.send_response(200)
                parent.send_header("Content-Type", "application/json")
                parent.end_headers()
                var json_str = self._json_module.dumps(resp).encode(
                    encoding="utf_8"
                )
                parent.wfile.write(json_str)

            # TODO: Add error handling.
            cpython.PyGILState_Release(state)

        try:
            # TODO: Fold into the ProtocolHandler eventually.
            self.handle_openai[handle, req_type](request)
        except e:
            response.set[Error](e)


def serve[EncodingT: QuantizationEncoding](config: Config):
    session_options = SessionOptions(
        _device=_Device.CUDA if config.use_gpu else _Device.CPU
    )
    session = InferenceSession(session_options)
    server = InferenceServer.create[True]("0.0.0.0:8000", session)
    service = Llama3InferenceService[EncodingT](
        config, server._impl._impl._ptr, session
    )
    print("Listening on port 8000!")
    service.init(server)
    server.serve(service)
    _ = server^
    _ = service^


@value
struct ServingWeightMetadata:
    var main: llama_main
    var url: String

    @staticmethod
    def _ALL() -> Dict[String, ServingWeightMetadata]:
        all_metadata = Dict[String, ServingWeightMetadata]()
        all_metadata[Q4_0Encoding.id()] = ServingWeightMetadata(
            serve[Q4_0Encoding],
            "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF/resolve/main/Meta-Llama-3-8B.Q4_0.gguf",
        )
        all_metadata[Q4_KEncoding.id()] = ServingWeightMetadata(
            serve[Q4_KEncoding],
            "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
        )
        all_metadata[Q6_KEncoding.id()] = ServingWeightMetadata(
            serve[Q6_KEncoding],
            "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q6_K.gguf",
        )
        return all_metadata^

    @staticmethod
    def from_encoding(encoding_id: String) -> Self:
        return Self._ALL()[encoding_id]


def llama3_serve():
    config = Config()
    set_locale_unicode()

    encoding = config.quantization_encoding.lower()
    weight_metadata = ServingWeightMetadata.from_encoding(encoding)
    if not str(config.model_path):
        # TODO(MOCO-794): assign directly
        var model_path: String = str(cached_weights(weight_metadata.url))
        config = Config()
        config.model_path = model_path
    weight_metadata.main(config)
