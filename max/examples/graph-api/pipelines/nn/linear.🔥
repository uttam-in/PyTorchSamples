# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

"""Module containing layers related to linear transformations."""

from max.graph import ops, Symbol
from max.graph.quantization import (
    Float32Encoding,
    QuantizationEncoding,
    Q4_0Encoding,
    Q4_KEncoding,
    Q6_KEncoding,
)


@value
struct Linear:
    """A fully-connected layer without activations."""

    var w: Symbol
    var encoding_id: String

    def __init__(inout self, w: Symbol):
        self.w = w
        # Default to float32 dtype if not provided.
        self.encoding_id = Float32Encoding.id()

    def __init__(inout self, encoded_weight: Tuple[Symbol, String]):
        self.w, self.encoding_id = encoded_weight

    def __call__(self, x: Symbol) -> Symbol:
        if self.encoding_id == Q4_0Encoding.id():
            return ops.qmatmul[Q4_0Encoding](x, self.w)
        elif self.encoding_id == Q4_KEncoding.id():
            return ops.qmatmul[Q4_KEncoding](x, self.w)
        elif self.encoding_id == Q6_KEncoding.id():
            return ops.qmatmul[Q6_KEncoding](x, self.w)
        elif self.encoding_id == Float32Encoding.id():
            return x @ self.w

        raise "unsupported quantization encoding in Linear: " + self.encoding_id

    def __rmatmul__(self, lhs: Symbol) -> Symbol:
        return self(lhs)
