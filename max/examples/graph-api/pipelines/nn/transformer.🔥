# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #

"""Neural network layers core to transformers."""

from max.graph import ops, Symbol
from max.graph.quantization import Float32Encoding, QuantizationEncoding

from pipelines.nn import Attention, Embedding, Linear, RMSNorm


@value
struct FeedForward:
    var w1: Linear
    var w2: Linear
    var w3: Linear

    def __call__(self, input: Symbol) -> Symbol:
        return (ops.silu(input @ self.w1) * (input @ self.w3)) @ self.w2


@value
struct TransformerBlock(CollectionElement):
    var attention: Attention
    var feed_forward: FeedForward
    var attention_norm: RMSNorm
    var ffn_norm: RMSNorm

    def __call__(
        self,
        input: Symbol,
        freqs_cis: Symbol,
        k_cache: Symbol,
        v_cache: Symbol,
    ) -> (Symbol, Symbol, Symbol):
        attention_out, k_cache_update, v_cache_update = self.attention(
            self.attention_norm(input), freqs_cis, k_cache, v_cache
        )
        h = input + attention_out
        h = h + self.feed_forward(self.ffn_norm(h))
        return h, k_cache_update, v_cache_update


@value
struct Transformer:
    alias max_seq_len = 2048

    var dim: Int
    var n_heads: Int

    var embedding: Embedding
    var layers: List[TransformerBlock]
    var norm: RMSNorm
    var output: Linear
    var theta: Float64

    def freqs_cis(self, start_pos: Symbol, seq_len: Symbol) -> Symbol:
        g = start_pos.graph()
        n = self.dim // self.n_heads
        iota = g.range[DType.float32](0, n - 1, 2)
        freqs = 1.0 / (self.theta ** (iota / n))
        t = g.range[DType.float32](0, Self.max_seq_len * 2.0, 1)
        freqs = t.reshape(-1, 1) * freqs.reshape(1, -1)

        var retval = ops.stack(List(ops.cos(freqs), ops.sin(freqs)), axis=-1)
        return ops.cast(retval[start_pos : start_pos + seq_len], DType.float32)

    def __call__(
        self, tokens: Symbol, k_cache: Symbol, v_cache: Symbol
    ) -> (Symbol, Symbol, Symbol):
        g = tokens.graph()

        start_pos = ops.shape_of(k_cache)[0]
        h = self.embedding(tokens)
        freqs_cis = self.freqs_cis(start_pos, ops.shape_of(tokens)[1])

        k_cache_updates = List[Symbol]()
        v_cache_updates = List[Symbol]()
        for i in range(len(self.layers)):
            h, k_cache_layer_update, v_cache_layer_update = self.layers[i](
                h,
                freqs_cis,
                ops.slice[keep_dims=True](k_cache, g.scalar(Int64(i)), axis=1),
                ops.slice[keep_dims=True](v_cache, g.scalar(Int64(i)), axis=1),
            )
            k_cache_updates.append(k_cache_layer_update.swapaxes(0, 1))
            v_cache_updates.append(v_cache_layer_update.swapaxes(0, 1))

        return (
            self.norm(h) @ self.output,
            ops.stack(k_cache_updates, axis=1),
            ops.stack(v_cache_updates, axis=1),
        )
