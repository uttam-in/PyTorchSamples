# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Pipeline for quantizing a Llama model trained on TinyStories."""

from pathlib import cwd, Path

from max.graph import Dim, Graph, Symbol, TensorType, Type
from max.graph.checkpoint import save, TensorDict
from max.graph.quantization import Q4_0Encoding
from tensor import Tensor, TensorShape

from pipelines.llama2.run import compile_graph, generate_text, Config
from pipelines.llama3.metrics import Metrics
from pipelines.nn import (
    Attention,
    Embedding,
    FeedForward,
    RMSNorm,
    Transformer,
    TransformerBlock,
)
from pipelines.weights.download import download_weights_to_cache
from pipelines.weights.llama2checkpoint import LlamaCFile
from pipelines.weights.loadable_model import LlamaHParams


@always_inline
def param_key(name: String, layer_idx: Optional[Int] = None) -> String:
    """Qualify parameter name with its layer index, if passed."""
    return name + "_" + str(layer_idx.value()[]) if layer_idx else name


def add_hyperparams_to_dict(
    inout tensor_dict: TensorDict, hyperparams: LlamaHParams
):
    """Copies all hyperparameters into a TensorDict for later checkpointing."""
    tensor_dict.set(
        "hyperparams.dims",
        Tensor[DType.int32](TensorShape(1), hyperparams.dims),
    )
    tensor_dict.set(
        "hyperparams.n_layers",
        Tensor[DType.int32](TensorShape(1), hyperparams.n_layers),
    )
    tensor_dict.set(
        "hyperparams.n_heads",
        Tensor[DType.int32](TensorShape(1), hyperparams.n_heads),
    )
    tensor_dict.set(
        "hyperparams.vocab_size",
        Tensor[DType.int32](TensorShape(1), hyperparams.vocab_size),
    )
    tensor_dict.set(
        "hyperparams.norm_eps",
        Tensor[DType.float64](TensorShape(1), hyperparams.norm_eps),
    )
    tensor_dict.set(
        "hyperparams.n_kv_heads",
        Tensor[DType.int32](TensorShape(1), hyperparams.n_kv_heads),
    )


struct TeenyTinyLlama:
    """Builder for a teeny tiny Llama 2 model trained on TinyStories."""

    alias batch_size = 1

    var params_file: LlamaCFile
    """Checkpoint file containing float32 Llama 2 weights."""

    var hyperparams: LlamaHParams
    """Llama 2 hyperparameters, read from the checkpoint."""

    var quantized_params: TensorDict
    """Dictionary of quantized model parameters for checkpointing."""

    def __init__(inout self, model_path: Path):
        """Initializes float32 model parameters from `model_path`."""
        self.params_file = LlamaCFile(model_path)

        # Read Llama hyperparameters from the checkpoint.
        self.hyperparams = self.params_file.hyperparams()

        # Initialize an empty set of parameters to checkpoint post quantization.
        self.quantized_params = TensorDict()
        add_hyperparams_to_dict(self.quantized_params, self.hyperparams)

    def build(inout self) -> Graph:
        """Build the Llama 2 graph, quantizing its weights by construction."""
        # Set the KV cache and tokens input types.
        cache_type = TensorType(
            DType.float32,
            Dim.dynamic(),
            self.hyperparams.n_layers,
            Self.batch_size,
            self.hyperparams.n_kv_heads,
            self.hyperparams.head_dim,
        )
        tokens_type = TensorType(DType.int64, Self.batch_size, Dim.dynamic())
        llama = Graph(
            "TeenyTinyLlama", List[Type](tokens_type, cache_type, cache_type)
        )

        @parameter
        def quantize(
            name: String, layer_idx: Optional[Int] = None
        ) -> (Symbol, String):
            """Quantizes the parameter called `name` in the Llama 2 checkpoint.

            This function does the following:

            1. Quantizes the float32 parameter using the Q4_0 quantization
               encoding.
            2. Saves the resulting quantized parameter to this builder's
               parameter dictionary `self.quantized_params`.
               This is used to write out the checkpoint after graph building.
            3. Stages the quantized parameter as a constant op in the graph.
            """
            # 1. Quantize the parameter.
            q4_0_param = Q4_0Encoding.quantize(
                self.params_file.get[DType.float32](name, layer_idx)
            )

            # 2. Save the result in the parameter dictionary.
            self.quantized_params.set(
                key=param_key(name, layer_idx), value=q4_0_param
            )

            # 3. Stage the quantized constant op.
            return llama.constant(q4_0_param), Q4_0Encoding.id()

        @parameter
        def weight(name: String, layer_idx: Optional[Int] = None) -> Symbol:
            """Stages a float32 parameter as a constant op in the graph."""
            # Load a float32 parameter from the Llama 2 checkpoint.
            float32_param = self.params_file.get[DType.float32](name, layer_idx)
            self.quantized_params.set(
                key=param_key(name, layer_idx), value=float32_param
            )

            # Stage a float32 constant op in the graph.
            return llama.constant(float32_param)

        # Stage all of the transformer blocks in the stories15M Llama.
        transformer_blocks = List[TransformerBlock]()
        for layer_idx in range(self.hyperparams.n_layers):
            # Stage a transformer block with quantized weights.
            # Read in float32 weights and quantize them before staging the op.
            # In the process, save references to the quantized tensors in the
            # parameter dictionary.
            # Doing so enables saving the checkpoint after building the graph,
            # complete with quantized weights.
            block = TransformerBlock(
                attention=Attention(
                    n_heads=self.hyperparams.n_heads,
                    n_kv_heads=self.hyperparams.n_kv_heads,
                    head_dim=self.hyperparams.head_dim,
                    dim=self.hyperparams.dims,
                    enable_custom_rope_kernel=False,
                    use_custom_attention=True,
                    # Quantize all QKV and output projection matrices in the
                    # attention layer.
                    wq=quantize("attn_q", layer_idx),
                    wk=quantize("attn_k", layer_idx),
                    wv=quantize("attn_v", layer_idx),
                    wo=quantize("attn_output", layer_idx),
                ),
                feed_forward=FeedForward(
                    # Also quantize the feedforward gate/down/up matrices.
                    w1=quantize("ffn_gate", layer_idx),
                    w2=quantize("ffn_down", layer_idx),
                    w3=quantize("ffn_up", layer_idx),
                ),
                attention_norm=RMSNorm(
                    # Use float32 norm weights rather than quantizing them
                    # since they are bias vectors and therefore small.
                    self.hyperparams.norm_eps,
                    weight("attn_norm", layer_idx),
                ),
                ffn_norm=RMSNorm(
                    self.hyperparams.norm_eps,
                    # Also save feedforward layer norm weights as float32.
                    weight("ffn_norm", layer_idx),
                ),
            )
            transformer_blocks.append(block)

        # Stage the Llama 2 transformer model.
        var logits: Symbol
        k_cache = llama[1]
        v_cache = llama[2]
        logits, k_cache, v_cache = Transformer(
            dim=self.hyperparams.dims,
            n_heads=self.hyperparams.n_heads,
            embedding=Embedding(quantize("token_embd")),
            layers=transformer_blocks,
            norm=RMSNorm(self.hyperparams.norm_eps, weight("output_norm")),
            # For stories15M, output and token embeddings are shared.
            # So reuse the token embeddings for the output layer.
            output=quantize("token_embd"),
            theta=10000.0,
        )(tokens=llama[0], k_cache=k_cache, v_cache=v_cache)

        # Stage an output op for the computed logits and updated KV cache.
        llama.output(
            List[Symbol](
                logits[-1, axis=1].reshape(Self.batch_size, -1),
                k_cache,
                v_cache,
            )
        )

        # Return the constructed quantized Llama 2.
        return llama


def quantize_and_save_tinystories(checkpoint_path: Path):
    """Runs the quantize TinyStories pipeline."""
    # Download and cache the weights for Llama trained on TinyStories.
    cache_path = cwd().joinpath(".cache")
    download_weights_to_cache(
        cache_path,
        "https://github.com/tairov/llama2.mojo/raw/master/tokenizer.bin",
        "https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin",
    )

    # Stage the Llama model graph.
    llama_builder = TeenyTinyLlama(cache_path.joinpath("stories15M.bin"))
    llama = llama_builder.build()

    save(
        llama_builder.quantized_params,
        checkpoint_path,
    )

    # Generate text using the quantized Llama model and the provided prompt.
    metrics = Metrics()
    generate_text(
        compile_graph(llama),
        llama_builder.hyperparams,
        Config(tokenizer_path=cache_path.joinpath("tokenizer.bin")),
        metrics,
    )
    print()
    metrics.print()
