# ===----------------------------------------------------------------------=== #
# Copyright (c) 2024, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Pipeline for loading a quantized Llama model trained on TinyStories.

The code is almost identical to `quantize_tinystories.ðŸ”¥` except instead of
loading and quantizing weights from the karpathy/llama.c file, this loads the
already-quantized weights from the MAX checkpoint saved by the first run of the
pipeline.
"""

from pathlib import cwd, Path

from max.graph import ops, Dim, Graph, Symbol, TensorType, Type
from max.graph.checkpoint import load, TensorDict
from max.graph.quantization import Float32Encoding, Q4_0Encoding

from pipelines.llama2.run import compile_graph, generate_text, Config
from pipelines.llama3.metrics import Metrics
from pipelines.nn import (
    Attention,
    Embedding,
    FeedForward,
    RMSNorm,
    Transformer,
    TransformerBlock,
)
from pipelines.weights.download import download_weights_to_cache
from pipelines.weights.loadable_model import LlamaHParams


@always_inline
def param_key(name: String, layer_idx: Optional[Int] = None) -> String:
    """Qualify parameter name with its layer index, if passed."""
    return name + "_" + str(layer_idx.value()[]) if layer_idx else name


def read_hyperparams_from_dict(
    tensor_dict: TensorDict,
) -> LlamaHParams:
    dims = tensor_dict.get[DType.int32]("hyperparams.dims")[0]
    n_layers = tensor_dict.get[DType.int32]("hyperparams.n_layers")[0]
    n_heads = tensor_dict.get[DType.int32]("hyperparams.n_heads")[0]
    norm_eps = tensor_dict.get[DType.float64]("hyperparams.norm_eps")[0]
    n_kv_heads = tensor_dict.get[DType.int32]("hyperparams.n_kv_heads")[0]
    vocab_size = tensor_dict.get[DType.int32]("hyperparams.vocab_size")[0]
    return LlamaHParams(
        dims=int(dims),
        n_layers=int(n_layers),
        n_heads=int(n_heads),
        norm_eps=norm_eps,
        n_kv_heads=int(n_kv_heads),
        vocab_size=int(vocab_size),
        head_dim=int(dims // n_heads),
        n_rep=int(n_heads // n_kv_heads),
    )


struct TeenyTinyLlama:
    """Builder for a teeny tiny Llama 2 model trained on TinyStories."""

    alias batch_size = 1

    var hyperparams: LlamaHParams
    """Llama 2 hyperparameters, read from the checkpoint."""

    var quantized_params: TensorDict
    """Dictionary of quantized model parameters for checkpointing."""

    def __init__(inout self, model_path: Path):
        # Load quantized weights from MAX checkpoint.
        self.quantized_params = load(model_path)
        # Read Llama hyperparameters from the checkpoint.
        self.hyperparams = read_hyperparams_from_dict(self.quantized_params)

    def build(inout self) -> Graph:
        """Build the Llama 2 graph using the quantized weights from checkpoint.
        """
        # Set the KV cache and tokens input types.
        cache_type = TensorType(
            DType.float32,
            Dim.dynamic(),
            self.hyperparams.n_layers,
            Self.batch_size,
            self.hyperparams.n_kv_heads,
            self.hyperparams.head_dim,
        )
        tokens_type = TensorType(DType.int64, Self.batch_size, Dim.dynamic())
        llama = Graph(
            "TeenyTinyLlama", List[Type](tokens_type, cache_type, cache_type)
        )

        @parameter
        def quantized_weight(
            name: String, layer_idx: Optional[Int] = None
        ) -> (Symbol, String):
            """Stages a quantized parameter as a constant op in the graph."""
            # Load a parameter from the MAX checkpoint.
            param = self.quantized_params.get[DType.uint8](
                param_key(name, layer_idx)
            )

            # Stage a constant op in the graph and return it with its encoding.
            return llama.constant(param), Q4_0Encoding.id()

        @parameter
        def norm_weight(
            name: String, layer_idx: Optional[Int] = None
        ) -> Symbol:
            """Stages a norm weight parameter as a constant op in the graph."""
            return llama.constant(
                self.quantized_params.get[DType.float32](
                    param_key(name, layer_idx)
                )
            )

        # Stage all of the transformer blocks in the stories15M Llama.
        transformer_blocks = List[TransformerBlock]()
        for layer_idx in range(self.hyperparams.n_layers):
            # Stage a transformer block with quantized weights.
            block = TransformerBlock(
                attention=Attention(
                    n_heads=self.hyperparams.n_heads,
                    n_kv_heads=self.hyperparams.n_kv_heads,
                    head_dim=self.hyperparams.head_dim,
                    dim=self.hyperparams.dims,
                    enable_custom_rope_kernel=False,
                    use_custom_attention=True,
                    # Load the quantized attention layer weights for all QKV and
                    # output projection
                    wq=quantized_weight("attn_q", layer_idx),
                    wk=quantized_weight("attn_k", layer_idx),
                    wv=quantized_weight("attn_v", layer_idx),
                    wo=quantized_weight("attn_output", layer_idx),
                ),
                feed_forward=FeedForward(
                    # Also load the quantized feedforward gate/down/up matrices.
                    w1=quantized_weight("ffn_gate", layer_idx),
                    w2=quantized_weight("ffn_down", layer_idx),
                    w3=quantized_weight("ffn_up", layer_idx),
                ),
                attention_norm=RMSNorm(
                    # Load the float32 norm weights (were not quantized since
                    # they are bias vectors and therefore small).
                    self.hyperparams.norm_eps,
                    norm_weight("attn_norm", layer_idx),
                ),
                ffn_norm=RMSNorm(
                    self.hyperparams.norm_eps,
                    # Same for the feedforward layer norm weights.
                    norm_weight("ffn_norm", layer_idx),
                ),
            )
            transformer_blocks.append(block)

        # Stage the Llama 2 transformer model.
        var logits: Symbol
        k_cache = llama[1]
        v_cache = llama[2]
        logits, k_cache, v_cache = Transformer(
            dim=self.hyperparams.dims,
            n_heads=self.hyperparams.n_heads,
            embedding=Embedding(quantized_weight("token_embd")),
            layers=transformer_blocks,
            norm=RMSNorm(self.hyperparams.norm_eps, norm_weight("output_norm")),
            # For stories15M, output and token embeddings are shared.
            # So reuse the token embeddings for the output layer.
            output=quantized_weight("token_embd"),
            theta=10000.0,
        )(tokens=llama[0], k_cache=k_cache, v_cache=v_cache)

        # Stage an output op for the computed logits and updated KV cache.
        llama.output(
            List[Symbol](
                logits[-1, axis=1].reshape(Self.batch_size, -1),
                k_cache,
                v_cache,
            )
        )

        # Return the constructed quantized Llama 2.
        return llama


def load_quantized_tinystories(checkpoint_path: Path):
    """Runs the TinyStories pipeline."""
    # Download and cache only the tokenizer config for Llama.
    cache_path = cwd().joinpath(".cache")
    download_weights_to_cache(
        cache_path,
        "https://github.com/tairov/llama2.mojo/raw/master/tokenizer.bin",
    )

    # Stage the Llama model graph from the saved MAX checkpoint file.
    llama_builder = TeenyTinyLlama(checkpoint_path)
    llama = llama_builder.build()

    # Generate text using the quantized Llama model and the provided prompt.
    metrics = Metrics()
    generate_text(
        compile_graph(llama),
        llama_builder.hyperparams,
        Config(tokenizer_path=cache_path.joinpath("tokenizer.bin")),
        metrics,
    )
    print()
    metrics.print()
